[TOC]

### golang源码分析

#### golang的汇编引导过程

编译二进制文件

```bash
$ CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o ./gateway -gcflags '-l -N' ./cmd/gateway
```

` --gcflags '-l -N'` 编译时禁止内联和优化

二进制文件导出汇编代码,比如

```bash
$ go tool objdump gateway >> gateway.dump
```

- 入口文件:go/go1.13/src/runtime/rt0_linux_amd64.s

```assembly
TEXT _rt0_amd64_linux(SB)  /home/znddzxx112/local/go/go1.13/src/runtime/rt0_linux_amd64.s
rt0_linux_amd64.s:8	0x460bf0		e9fbc5ffff		JMP _rt0_amd64(SB)	
```

`JMP` 汇编语言跳转指令

`go1.13/src/runtime` 文件夹下有许多 rt0_开头的文件，不同OS和ARCH时入口文件不同

- 源码文件:go/go1.13/src/runtime/asm_amd64.s

```assembly
TEXT _rt0_amd64(SB) /home/znddzxx112/local/go/go1.13/src/runtime/asm_amd64.s
  asm_amd64.s:15	0x45d1f0		488b3c24		MOVQ 0(SP), DI		
  asm_amd64.s:16	0x45d1f4		488d742408		LEAQ 0x8(SP), SI	
  asm_amd64.s:17	0x45d1f9		e902000000		JMP runtime.rt0_go(SB)
```

继续查看相同文件go/go1.13/src/runtime/asm_amd64.s下runtime.rt0_go(SB)

```assembly
TEXT runtime.rt0_go(SB) /home/znddzxx112/local/go/go1.13/src/runtime/asm_amd64.s
    ...
    CALL	runtime·args(SB)
	CALL	runtime·osinit(SB)
	CALL	runtime·schedinit(SB)

	// create a new goroutine to start program
	MOVQ	$runtime·mainPC(SB), AX		// entry
	PUSHQ	AX
	PUSHQ	$0			// arg size
	CALL	runtime·newproc(SB)
	POPQ	AX
	POPQ	AX

	// start this M
	CALL	runtime·mstart(SB)

	CALL	runtime·abort(SB)	// mstart should never return
	RET

	// Prevent dead-code elimination of debugCallV1, which is
	// intended to be called by debuggers.
	MOVQ	$runtime·debugCallV1(SB), AX
	RET

DATA	runtime·mainPC+0(SB)/8,$runtime·main(SB)
GLOBL	runtime·mainPC(SB),RODATA,$8
```

`CALL` 是汇编子程序调用指令，这里调用了许多重要的函数

`CALL runtime.schedinit(SB)` 很多重要的初始化工作，包括内存分配初始化，以后有计划分享内存分配的内容

`MOVQ	$runtime·mainPC(SB), AX		// entry` 这里往SB虚拟寄存器中赋值runtime.main函数的入口（proc.go中的main函数） `DATA	runtime·mainPC+0(SB)/8,$runtime·main(SB)`



`CALL	runtime·newproc(SB)`  创建第一个g, 设置g的函数入口地址为runtime.main, 文件位置proc.go

```
func newproc(siz int32, fn *funcval) {
	argp := add(unsafe.Pointer(&fn), sys.PtrSize)
	gp := getg()
	pc := getcallerpc()
	systemstack(func() {
		newproc1(fn, (*uint8)(argp), siz, gp, pc)
	})
}

func newproc1(fn *funcval, argp *uint8, narg int32, callergp *g, callerpc uintptr) {
    ...
    // g的函数入口地址为runtime.main
	newg.startpc = fn.fn
	...
}
```

`CALL	runtime·mstart(SB)`  先执行到`schedule()`  调度。随后会最先执行runtime.main函数，runtime.main函数中会调用函数`gcenable()` 开启打扫和清除二个g任务, 随后进入用户层是main.main入口

```
func gcenable() {
	// Kick off sweeping and scavenging.
	c := make(chan int, 2)
	go bgsweep(c) // 打扫
	go bgscavenge(c) // 清除
	<-c
	<-c
	memstats.enablegc = true // now that runtime is initialized, GC is okay
}
```

```
func mstart() {
	_g_ := getg()

	osStack := _g_.stack.lo == 0
	if osStack {
		// Initialize stack bounds from system stack.
		// Cgo may have left stack size in stack.hi.
		// minit may update the stack bounds.
		size := _g_.stack.hi
		if size == 0 {
			size = 8192 * sys.StackGuardMultiplier
		}
		_g_.stack.hi = uintptr(noescape(unsafe.Pointer(&size)))
		_g_.stack.lo = _g_.stack.hi - size + 1024
	}
	// Initialize stack guard so that we can start calling regular
	// Go code.
	_g_.stackguard0 = _g_.stack.lo + _StackGuard
	// This is the g0, so we can also call go:systemstack
	// functions, which check stackguard1.
	_g_.stackguard1 = _g_.stackguard0
	mstart1()

	// Exit this thread.
	if GOOS == "windows" || GOOS == "solaris" || GOOS == "illumos" || GOOS == "plan9" || GOOS == "darwin" || GOOS == "aix" {
		// Windows, Solaris, illumos, Darwin, AIX and Plan 9 always system-allocate
		// the stack, but put it in _g_.stack before mstart,
		// so the logic above hasn't set osStack yet.
		osStack = true
	}
	mexit(osStack)
}

```

```
func mstart1() {
	_g_ := getg()

	if _g_ != _g_.m.g0 {
		throw("bad runtime·mstart")
	}

	// Record the caller for use as the top of stack in mcall and
	// for terminating the thread.
	// We're never coming back to mstart1 after we call schedule,
	// so other calls can reuse the current frame.
	save(getcallerpc(), getcallersp())
	asminit()
	minit()

	// Install signal handlers; after minit so that minit can
	// prepare the thread to be able to handle the signals.
	if _g_.m == &m0 {
		mstartm0()
	}

	if fn := _g_.m.mstartfn; fn != nil {
		fn()
	}

	if _g_.m != &m0 {
		acquirep(_g_.m.nextp.ptr())
		_g_.m.nextp = 0
	}
	schedule()
}
```



- 源码文件：/go/go1.13/src/runtime/proc.go

```golang
func main() {
	...
	fn := main_main // make an indirect call, as the linker doesn't know the address of the main package when laying down the runtime
	fn()
    ...
}
//go:linkname main_main main.main
func main_main()
```

汇编代码的引导过程到这里就结束了，从这个函数开始都是用golang编写。下面是main函数的汇编代码

```assembly
TEXT runtime.main(SB) /home/znddzxx112/local/go/go1.13/src/runtime/proc.go
...
  proc.go:203		0x430cae		488b056331e400		MOVQ 0xe43163(IP), AX				
  proc.go:203		0x430cb5		488d155c31e400		LEAQ 0xe4315c(IP), DX				
  proc.go:203		0x430cbc		ffd0			CALL AX	
  ...
```

结合汇编代码，说下我的理解：（不保证100%正确）

`func main_main()` 在链接时将golang程序的主包main函数地址与main_main()关联起来，寄存器AX应该放置了main函数地址, 执行`CALL AX`时就会去golang程序中的main函数代码了

- 使用gdb单步调试

  ```bash
  $ gdb gateway
  (gdb) b runtime.main(SB)
  (gdb) r
  (gdb) n
  ```

  `n` 一直按n进行单步调试，这里有多线程没有单步至`fn()`处，调度器不断在进行调度。

- 使用gdb attach方式观察程序

  ```bash
  $ ./gateway
  $ gdb attach 10564
  (gdb) info threads
  (gdb) thread 6
  (gdb) info stack
  ```

  `info threads` 看到的时内核线程，即GPM中M

  `thread 6` 是切到具体线程

  `info stack` 查看线程的栈情况

- 入口调试

  ```
  $ gdb malloc
  (gdb) b runtime.rt0_go
  (gdb) b runtime.mstart
  (gdb) b runtime.main
  (gdb) b main.main 用户级别入口函数
  (gdb) n
  
  ```

  

#### 总结

​		引导过程先从asm_amd64.s中rt0_go开始，完成`runtime·args(SB)`, `runtime·osinit(SB)` `runtime·schedinit(SB)` , 并且创建第一个g任务，g开始函数入口指向proc.go文件中`runtime.main`  函数,  随后调用文件proc.go文件中`runtime·mstart` 开始启动调度器。

​		调度器首先会执行proc.go文件中`runtime.main`  函数,在这个函数中会`gcenable()` 启动二个g任务分别是垃圾标记和清除，随后执行用户入口函数main_main。

​		在进入用户入口函数main_main时，已有3个go协程在运行了

​		学习和分析golang的汇编引导过程，能窥见golang全貌，接下去就是逐一学习和领悟



#### Golang系统调用学习笔记

> `x86(-64)` 上共有`int 80`, `sysenter`, `syscall`三种方式来实现系统调用。`int 80` 是最传统的调用方式，其通过中断/异常来实现。`sysenter` 与 `syscall` 则都是通过引入新的寄存器组( Model-Specific Register(MSR))存放所需信息，进而实现快速跳转

> 发生系统调用，程序控制权交给内核，处于内核态
>
> 系统调用结束，控制权返回给程序，处于用户态

- 文件位置: go1.13/src/syscall/syscall_unix.go

```goalng
func Syscall(trap, a1, a2, a3 uintptr) (r1, r2 uintptr, err Errno)
func Syscall6(trap, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2 uintptr, err Errno)
func RawSyscall(trap, a1, a2, a3 uintptr) (r1, r2 uintptr, err Errno)
func RawSyscall6(trap, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2 uintptr, err Errno)
```

`trap` 代表中断号/系统调用号 可在文件`go1.13/src/syscall/zsysnum_linux_amd64.go`文件中查看

`a1 ~ a2` 为参数

- 文件位置: go1.13/src/syscall/asm_linux_amd64.go

  ```assembly
  // func Syscall(trap int64, a1, a2, a3 uintptr) (r1, r2, err uintptr);
  // Trap # in AX, args in DI SI DX R10 R8 R9, return in AX DX
  TEXT ·Syscall(SB),NOSPLIT,$0-56
  	CALL	runtime·entersyscall(SB)
  	MOVQ	a1+8(FP), DI
  	MOVQ	a2+16(FP), SI
  	MOVQ	a3+24(FP), DX
  	MOVQ	$0, R10
  	MOVQ	$0, R8
  	MOVQ	$0, R9
  	MOVQ	trap+0(FP), AX	// syscall entry
  	SYSCALL
  	CMPQ	AX, $0xfffffffffffff001
  	JLS	ok
  	MOVQ	$-1, r1+32(FP)
  	MOVQ	$0, r2+40(FP)
  	NEGQ	AX
  	MOVQ	AX, err+48(FP)
  	CALL	runtime·exitsyscall(SB)
  	RET
  ok:
  	MOVQ	AX, r1+32(FP)
  	MOVQ	DX, r2+40(FP)
  	MOVQ	$0, err+48(FP)
  	CALL	runtime·exitsyscall(SB)
  	RET
  ```

  `Syscall`系统调用Syscall的实现，可以窥见传递参数和系统调用号后调用`SYSCALL`，系统调用结束后，获取返回值并调用`runtime·exitsyscall(SB)`

  再看RawSyscall()，与Syscall()少了`CALL	runtime·entersyscall(SB)`和`runtime·exitsyscall(SB)`

  ```
  // func RawSyscall(trap, a1, a2, a3 uintptr) (r1, r2, err uintptr)
  TEXT ·RawSyscall(SB),NOSPLIT,$0-56
  	MOVQ	a1+8(FP), DI
  	MOVQ	a2+16(FP), SI
  	MOVQ	a3+24(FP), DX
  	MOVQ	$0, R10
  	MOVQ	$0, R8
  	MOVQ	$0, R9
  	MOVQ	trap+0(FP), AX	// syscall entry
  	SYSCALL
  	CMPQ	AX, $0xfffffffffffff001
  	JLS	ok1
  	MOVQ	$-1, r1+32(FP)
  	MOVQ	$0, r2+40(FP)
  	NEGQ	AX
  	MOVQ	AX, err+48(FP)
  	RET
  ok1:
  	MOVQ	AX, r1+32(FP)
  	MOVQ	DX, r2+40(FP)
  	MOVQ	$0, err+48(FP)
  	RET
  ```

#### 封装系统调用

```golang
pid, _, _ := syscall.Syscall(39, 0, 0, 0) // 调用39号SYS_GETPID系统调用
fmt.Println("Process id: ", pid)
```

还可以直接使用 `os.Getpid() 或者 syscall.Getpid()`

再看下`os.Getpid() `源码

```golang
func Getpid() int { return syscall.Getpid() }
```

#### 总结

学习和分析golang系统调用，能得到编程启迪



## 内存管理

参考文章

> 非常棒的分析文章:https://www.debug8.com/golang/t_51650.html
>
> https://blog.csdn.net/u010853261/article/details/102945046

### 内存初始化过程

asm_amd64.s rt0_go程序

```
CALL	runtime·args(SB)
CALL	runtime·osinit(SB)
CALL	runtime·schedinit(SB)

// create a new goroutine to start program
MOVQ	$runtime·mainPC(SB), AX		// entry
```

os_linux.go 

```
func osinit() {
	ncpu = getproccount() // 获取cpu的核数量
	physHugePageSize = getHugePageSize() // 系统给线程分配页的大小 一般为4KB
}
```

文件位置：proc.go

```
func schedinit()  {
		...
		mallocinit()
		...
}
```

golang在运行时申请虚拟内存

#### mallocinit

通过`sysReserve` 向系统申请一块连续的内存 `spans+bitmap+arena`。其中arena为各个级别缓存结构提供的分配的内存块，spans是个指针数组用来按照page寻址arena区域。

> 最终sysReserve调用的是系统调用`mmap`。申请了512GB的虚拟地址空间，真正的物理内存则是用到的时候发生缺页才真实占用的。

```
func mallocinit() {
	 ...
	mheap_.init()
	_g_ := getg()  // 获取当前G
	_g_.m.mcache = allocmcache() // _g_.m 当前G分配mcache
	
	if sys.PtrSize == 8 { // 64位机器上
	        ...
	        for i := 0x7f; i >= 0; i-- {
	        // mheap中arnea区域初始化 
			hint := (*arenaHint)(mheap_.arenaHintAlloc.alloc())
			hint.addr = p
			hint.next, mheap_.arenaHints = mheap_.arenaHints, hint
			}
	}	
	
	...
}
	
```

> 0x7f = 128 
>
> 分配128个heapArea,每个heapArea是64MB，总共 128 * 64MB = 8GB的空间  2^13 MB
>
> 每个heapArea基地址保存在arenaHints列表结构中
>
> 这里分配的虚拟地址空间，在发生



#### 核心结构体

> heapArena: 保留整个虚拟地址空间
> mspan：是 mheap 上管理的一连串的页
> mheap：分配的堆，在页大小为 8KB 的粒度上进行管理
> mcentral：搜集了给定大小等级的所有 span
> mcache：为 per-P 的缓存。
> 页是向操作系统申请内存的最小单位，目前设计为 8kb。
>
> 在golang里面内存分为部分，传统意义上的栈由 runtime 统一管理，用户态不感知。而传统意义上的堆内存，又被 Go 运行时划分为了两个部分，
>
> 一个是 Go 运行时自身所需的堆内存，即堆外内存；
> 另一部分则用于 Go 用户态代码所使用的堆内存，也叫做 Go 堆。
> Go 堆负责了用户态对象的存放以及 goroutine 的执行栈。

#### mheap结构

golang内存管理使用mheap来管理，主要是6种区域arena、free、large、sweep、center、fixalloc

注解：我认为tcmalloc方式，使用不同规格的大小相同的块来使内存分配和回收合理，mheap是放在程序数据段，mheap管理的6个区域是在系统分配的空间上，传统意义的堆上。在系统分配的空间上形成go堆。

##### arena区域

```
type mheap struct {
	arenas [1 << arenaL1Bits]*[1 << arenaL2Bits]*heapArena
	allArenas []arenaIdx
	sweepArenas []arenaIdx
	curArena struct {
		base, end uintptr
	}
	arenaHints *arenaHint
}
type arenaIdx uint
```

> arenas 存储于go堆，并通过allArenas,sweepArenas索引访问，是一个二维数组
>
> arenaIdx 通过 l1()和l2()方法获取 二维数组行列数值
>
> Golang 的堆由很多个 arena 组成，每个 arena 在 64 位机器上是 64MB

```golang
const(
pageSize = 8192//8KB
heapArenaBytes = 67108864 //一个heapArena是64MB
heapArenaBitmapBytes = heapArenaBytes / 32 // 一个heapArena的bitmap占用2MB
pagesPerArena = heapArenaBytes / pageSize  // 一个heapArena包含8192个页 8k个页
)
type heapArena struct {
	bitmap [heapArenaBitmapBytes]byte //2,097,152  2MB 是一个2MB个byte数组来标记这个heap area 64M 内存的使用情况
	spans [pagesPerArena]*mspan / 8k个mspan,每个mspan是8kB,一个heapArena总共可用64MB空间
	pageInUse [pagesPerArena / 8]uint8 //是一个位图，使用1024 * 8 bit来标记 8192个页(8192*8KB = 64MB)中哪些页正在使用中
	pageMarks [pagesPerArena / 8]uint8
}
type arenaHint struct {
	addr uintptr
	down bool
	next *arenaHint
}
```

heapArena区域 描述了一个 heap arena 的元信息。

> bitmap = 2* 1024 * 1024 * 8 bit = 2MB 
>
> 按照1bit能表示1个Byte是否使用，所以 2MB的bitmap能标识 8*1024 * 8 * 1024 Byte = 64MB的空间
>
> arenaHint 是实际分配，链表形式存在

是 arenaHint 链表的节点结构，保存了arena 的起始地址、是否为最后一个 arena，以及下一个 arenaHint 指针。

hint指向了指向每个arena基地址

```
 // mheap中arnea区域初始化 
hint := (*arenaHint)(mheap_.arenaHintAlloc.alloc())
hint.addr = p
hint.next, mheap_.arenaHints = mheap_.arenaHints, hint
```



```
type mheap struct {
	 free      mTreap // free spans
}
type mTreap struct {
	treap           *treapNode
	unscavHugePages uintptr // number of unscavenged huge pages in the treap
}
type treapNode struct {
	right    *treapNode      // all treapNodes > this treap node
	left     *treapNode      // all treapNodes < this treap node
	parent   *treapNode      // direct parent of this node, nil if root
	key      uintptr         // base address of the span, used as primary sort key
	span     *mspan          // span at base address key
	maxPages uintptr         // the maximum size of any span in this subtree, including the root
	priority uint32          // random number used by treap algorithm to keep tree probabilistically balanced
	types    treapIterFilter // the types of spans available in this subtree
}
```

free区域稍后讲解



```
type mheap struct {
	// Malloc stats.
	largealloc  uint64                  // bytes allocated for large objects
	nlargealloc uint64                  // number of large object allocations
	largefree   uint64                  // bytes freed for large objects (>maxsmallsize)
	nlargefree  uint64                  // number of frees for large objects (>maxsmallsize)
}
```

large区域，存储大规格的对象。小对象都能存储在mspan上。稍后讲解



```
type mheap struct {
	sweepgen  uint32 // sweep generation, see comment in mspan
	sweepdone uint32 // all spans are swept
	sweepers  uint32 // number of active sweepone calls
	sweepSpans [2]gcSweepBuf
	pagesInUse         uint64  // pages of spans in stats mSpanInUse; R/W with mheap.lock
	pagesSwept         uint64  // pages swept this cycle; updated atomically
	pagesSweptBasis    uint64  // pagesSwept to use as the origin of the sweep ratio; updated atomically
	sweepHeapLiveBasis uint64  // value of heap_live to use as the origin of sweep ratio; written with lock, read without
	sweepPagesPerByte  float64 // proportional sweep ratio; written with lock, read without
}
```

> sweepSpans [2]gcSweepBuf 分为已打扫span和未打扫的span

sweep内存回收区域



```
type mheap struct {
	central [numSpanClasses]struct {
		mcentral mcentral
		pad      [cpu.CacheLinePadSize - unsafe.Sizeof(mcentral{})%cpu.CacheLinePadSize]byte
	}
	
}
```

numSpanClasses=134 = 67 * 2 每种类型分为有有指针和无指针

当 mcentral 中 nonempty 列表中也没有可分配的 span 时，则会向 mheap 提出请求，从而获得新的 span，并进而交给 mcache

center区域的初始化代码

```
for i := range h.central {
		h.central[i].mcentral.init(spanClass(i))
}
// Initialize a single central free list.
func (c *mcentral) init(spc spanClass) {
	c.spanclass = spc
	c.nonempty.init()
	c.empty.init()
}
```

#### mcenter结构体

```
type mcentral struct {
	lock      mutex
	spanclass spanClass
	nonempty  mSpanList // list of spans with a free object, ie a nonempty free list
	empty     mSpanList // list of spans with no free objects (or cached in an mcache)
	nmalloc uint64
}
```

#### mSpanList结构体

是一个链表

```
type mSpanList struct {
	first *mspan // first span in list, or nil if none
	last  *mspan // last span in list, or nil if none
}
```

总共134个mcenter,每个mcenter包含空闲和非空闲的mspan队列



```
type mheap struct {
	spanalloc             fixalloc // allocator for span*
	cachealloc            fixalloc // allocator for mcache*
	treapalloc            fixalloc // allocator for treapNodes*
	specialfinalizeralloc fixalloc // allocator for specialfinalizer*
	specialprofilealloc   fixalloc // allocator for specialprofile*
	speciallock           mutex    // lock for special record allocators.
	arenaHintAlloc        fixalloc // allocator for arenaHints
}
```

> fixalloc:分配固定大小的块

fixalloc区域



#### mheap_.init()

> 初始化Go堆，分配固定大小块

```golang
// Initialize the heap.
func (h *mheap) init() {
	h.treapalloc.init(unsafe.Sizeof(treapNode{}), nil, nil, &memstats.other_sys)
	h.spanalloc.init(unsafe.Sizeof(mspan{}), recordspan, unsafe.Pointer(h), &memstats.mspan_sys)
	h.cachealloc.init(unsafe.Sizeof(mcache{}), nil, nil, &memstats.mcache_sys)
	h.specialfinalizeralloc.init(unsafe.Sizeof(specialfinalizer{}), nil, nil, &memstats.other_sys)
	h.specialprofilealloc.init(unsafe.Sizeof(specialprofile{}), nil, nil, &memstats.other_sys)
	h.arenaHintAlloc.init(unsafe.Sizeof(arenaHint{}), nil, nil, &memstats.other_sys)

	h.spanalloc.zero = false

	// h->mapcache needs no init
	// mcenter初始化
	for i := range h.central {
		h.central[i].mcentral.init(spanClass(i))
	}
}
```



#### mcache结构

```
type mcache struct {
	alloc [numSpanClasses]*mspan // 67 * 2
}
```

每个mcache包含134个mspan

我们知道每个 Gorontine 的运行都是绑定到一个 P 上面，mcache 是每个 P 的 cache。这么做的好处是分配内存时不需要加锁

当 mcache 中 span 的数量不够使用时，会向 mcentral 的 nonempty 列表中获得新的 span。



#### mspan结构

runtime/mheap.go

```
type mspan stuct {
		next *mspan
		prev *mspan
		startAddr uintptr
		npages    uintptr
		spanclass   spanClass 
	    elemsize    uintptr
	    nelems uintptr
	    
		freeindex uintptr
		
		allocBits  *gcBits
	    gcmarkBits *gcBits
	    
	    sweepgen    uint32
	    
}
```

> startAddr 开始页地址
>
> npages   总共分配多少页，一个页大小是8KB
>
> spanclass   spanClass span存储的对象类型 1～67
> elemsize    uintptr   对象的大小
> nelems uintptr   最多能存储的对象个数  8kB/elemsize
>
> freeindex表示 <该位置的都被分配了, >=该位置的可能被分配, 也可能没有. 配合allocCache来寻找. 每次分配后, freeindex设置为分配的slot+1.
> allocBits表示上一次GC之后哪一些slot被使用了. 0未使用或释放, 1已分配.
> allocCache表示从freeindex开始的64个slot的分配情况, 1为未分配, 0为分配. 使 用ctz(Count Trailing Zeros指令)来找到第一个非0位. 使用完了就从allocBits加载, 取 反.
> 每次gc完之后, sweep阶段, 将allocBits设置为gcmarkBits.
>



#### 各种结构的关系

![20191113182433458](./20191113182433458.png)

分配的整体顺序是从右向左，代价也越来越大。

小对象和微对象优先从白色区域 per-P 的 mcache 分配 span，这个过程不需要加锁（白色）；
若失败则会从 mheap 持有的 mcentral 加锁获得新的 span，这个过程需要加锁，但只是局部（灰色）；
若仍失败则会从右侧的 free 或 scav 进行分配，这个过程需要对整个 heap 进行加锁，代价最大（黑色）。



### 内存分配过程

runtime/malloc.go

```golang
// 为一种类型返回存储指针
func newobject(typ *_type) unsafe.Pointer {
	return mallocgc(typ.size, typ, true) 
}
const (
	maxSmallSize = 32768 //32kb
	maxTinySize = 16 //16byte
)
// 为对象分配存储字节
// 小对象被分配到p的mcache free列表中
// 大对象直接分配到mheap中
func mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer {
		...
		c := gomcache()  // mcache
	  var x unsafe.Pointer   // 分配到的空间
	  noscan := typ == nil || typ.ptrdata == 0  // 是否需要分配到非扫描列表中
	  if size <= maxSmallSize {
		if noscan && size < maxTinySize {
				// 不需要扫描并且是tiny 对象
		} else {
				// small 对象
				size = uintptr(class_to_size[sizeclass])
			spc := makeSpanClass(sizeclass, noscan)
			span := c.alloc[spc]
			v := nextFreeFast(span)  // 看能否获取到空闲mspan，有则返回指针
			if v == 0 {
				v, span, shouldhelpgc = c.nextFree(spc) // 向mheap中mcenter获取mspan，会加mcenter局部锁
			}
			x = unsafe.Pointer(v)
		}
	} else {
		// 大对象直接分配到mheap
	}
}
```



如果从mspan中找到空闲位置，返回指针，否则返回0. 

此处可以看到mspan中许多字段的作用

```
// nextFreeFast returns the next free object if one is quickly available.
// Otherwise it returns 0.
func nextFreeFast(s *mspan) gclinkptr {
	theBit := sys.Ctz64(s.allocCache) // Is there a free object in the allocCache?
	if theBit < 64 {
		result := s.freeindex + uintptr(theBit)
		if result < s.nelems {
			freeidx := result + 1
			if freeidx%64 == 0 && freeidx != s.nelems {
				return 0
			}
			s.allocCache >>= uint(theBit + 1)
			s.freeindex = freeidx
			s.allocCount++
			return gclinkptr(result*s.elemsize + s.base())
		}
	}
	return 0
}
```

如果上面函数返回0，意味着mcache中此种类型的mspan已经没有空闲位置。会通过下面的nextFree()获取。nextFreeIndex()函数再次向mcache中查询有无空闲位置。

```
func (c *mcache) nextFree(spc spanClass) (v gclinkptr, s *mspan, shouldhelpgc bool) {
	s = c.alloc[spc]
	shouldhelpgc = false
	freeIndex := s.nextFreeIndex()
	if freeIndex == s.nelems {
		// The span is full.
		if uintptr(s.allocCount) != s.nelems {
			println("runtime: s.allocCount=", s.allocCount, "s.nelems=", s.nelems)
			throw("s.allocCount != s.nelems && freeIndex == s.nelems")
		}
		c.refill(spc)
		shouldhelpgc = true
		s = c.alloc[spc]

		freeIndex = s.nextFreeIndex()
	}

	if freeIndex >= s.nelems {
		throw("freeIndex is not valid")
	}

	v = gclinkptr(freeIndex*s.elemsize + s.base())
	s.allocCount++
	if uintptr(s.allocCount) > s.nelems {
		println("s.allocCount=", s.allocCount, "s.nelems=", s.nelems)
		throw("s.allocCount > s.nelems")
	}
	return
}
```

c.refill(spc) 会向mcenter中申请一个新的mspan，s变量就是新的mspan

```
	s = mheap_.central[spc].mcentral.cacheSpan()
```

原先满的mspan会被操作atomic.Store(&s.sweepgen, mheap_.sweepgen)

```
func (c *mcache) refill(spc spanClass) {
	// Return the current cached span to the central lists.
	s := c.alloc[spc]

	if uintptr(s.allocCount) != s.nelems {
		throw("refill of span with free space remaining")
	}
	if s != &emptymspan {
		// Mark this span as no longer cached.
		if s.sweepgen != mheap_.sweepgen+3 {
			throw("bad sweepgen in refill")
		}
		atomic.Store(&s.sweepgen, mheap_.sweepgen)
	}

	// Get a new cached span from the central lists.
	s = mheap_.central[spc].mcentral.cacheSpan()
	if s == nil {
		throw("out of memory")
	}

	if uintptr(s.allocCount) == s.nelems {
		throw("span has no free space")
	}

	// Indicate that this span is cached and prevent asynchronous
	// sweeping in the next sweep phase.
	s.sweepgen = mheap_.sweepgen + 3

	c.alloc[spc] = s
}
```

再看下mcentral是如何获取可用mspan

加锁，遍历noempty队列，如果有空闲，插入到empty队列尾部，返回mspan

遍历empty队列，如果有空闲，返回mspan

```golang
// Allocate a span to use in an mcache.
func (c *mcentral) cacheSpan() *mspan {
	// Deduct credit for this span allocation and sweep if necessary.
		spanBytes := uintptr(class_to_allocnpages[c.spanclass.sizeclass()]) * _PageSize
	deductSweepCredit(spanBytes, 0)

	lock(&c.lock)
	traceDone := false
	if trace.enabled {
		traceGCSweepStart()
	}
	sg := mheap_.sweepgen
retry:
	var s *mspan
	for s = c.nonempty.first; s != nil; s = s.next {
		if s.sweepgen == sg-2 && atomic.Cas(&s.sweepgen, sg-2, sg-1) {
			c.nonempty.remove(s)
			c.empty.insertBack(s)
			unlock(&c.lock)
			s.sweep(true)
			goto havespan
		}
		if s.sweepgen == sg-1 {
			// the span is being swept by background sweeper, skip
			continue
		}
		// we have a nonempty span that does not require sweeping, allocate from it
		c.nonempty.remove(s)
		c.empty.insertBack(s)
		unlock(&c.lock)
		goto havespan
	}

	for s = c.empty.first; s != nil; s = s.next {
		if s.sweepgen == sg-2 && atomic.Cas(&s.sweepgen, sg-2, sg-1) {
			// we have an empty span that requires sweeping,
			// sweep it and see if we can free some space in it
			c.empty.remove(s)
			// swept spans are at the end of the list
			c.empty.insertBack(s)
			unlock(&c.lock)
			s.sweep(true)
			freeIndex := s.nextFreeIndex()
			if freeIndex != s.nelems {
				s.freeindex = freeIndex
				goto havespan
			}
			lock(&c.lock)
			// the span is still empty after sweep
			// it is already in the empty list, so just retry
			goto retry
		}
		if s.sweepgen == sg-1 {
			// the span is being swept by background sweeper, skip
			continue
		}
		// already swept empty span,
		// all subsequent ones must also be either swept or in process of sweeping
		break
	}
	if trace.enabled {
		traceGCSweepDone()
		traceDone = true
	}
	unlock(&c.lock)

	// Replenish central list if empty.
	s = c.grow()
	if s == nil {
		return nil
	}
	lock(&c.lock)
	c.empty.insertBack(s)
	unlock(&c.lock)

	// At this point s is a non-empty span, queued at the end of the empty list,
	// c is unlocked.
havespan:
	if trace.enabled && !traceDone {
		traceGCSweepDone()
	}
	n := int(s.nelems) - int(s.allocCount)
	if n == 0 || s.freeindex == s.nelems || uintptr(s.allocCount) == s.nelems {
		throw("span has no free objects")
	}
	// Assume all objects from this span will be allocated in the
	// mcache. If it gets uncached, we'll adjust this.
	atomic.Xadd64(&c.nmalloc, int64(n))
	usedBytes := uintptr(s.allocCount) * s.elemsize
	atomic.Xadd64(&memstats.heap_live, int64(spanBytes)-int64(usedBytes))
	if trace.enabled {
		// heap_live changed.
		traceHeapAlloc()
	}
	if gcBlackenEnabled != 0 {
		// heap_live changed.
		gcController.revise()
	}
	freeByteBase := s.freeindex &^ (64 - 1)
	whichByte := freeByteBase / 8
	// Init alloc bits cache.
	s.refillAllocCache(whichByte)

	// Adjust the allocCache so that s.freeindex corresponds to the low bit in
	// s.allocCache.
	s.allocCache >>= s.freeindex % 64

	return s
}
```

如果mcenter中此时empty列表也没有可用mspan, 则使用grow()方法获取一个新的mspan,插入到empty列表队尾。

grow函数是如何増长出新的mspan的?

```
func (c *mcentral) grow() *mspan {
	npages := uintptr(class_to_allocnpages[c.spanclass.sizeclass()])
	size := uintptr(class_to_size[c.spanclass.sizeclass()])

	s := mheap_.alloc(npages, c.spanclass, false, true)
	if s == nil {
		return nil
	}

	// Use division by multiplication and shifts to quickly compute:
	// n := (npages << _PageShift) / size
	n := (npages << _PageShift) >> s.divShift * uintptr(s.divMul) >> s.divShift2
	s.limit = s.base() + size*n
	heapBitsForAddr(s.base()).initSpan(s)
	return s
}
```

可以看到调用mheap_.alloc(npages, c.spanclass, false, true)，向mheap请求分配得到的

```
func (h *mheap) alloc(npage uintptr, spanclass spanClass, large bool, needzero bool) *mspan {
	// Don't do any operations that lock the heap on the G stack.
	// It might trigger stack growth, and the stack growth code needs
	// to be able to allocate heap.
	var s *mspan
	systemstack(func() {
		s = h.alloc_m(npage, spanclass, large)
	})

	if s != nil {
		if needzero && s.needzero != 0 {
			memclrNoHeapPointers(unsafe.Pointer(s.base()), s.npages<<_PageShift)
		}
		s.needzero = 0
	}
	return s
}
```

mheap分配mspan的过程

首先通过`lock(&h.lock)` 全局上锁，`s := h.allocSpanLocked(npage, &memstats.heap_inuse)` 获取分配到mspan，随后将span信息sweepSpans中，为回收做准备

```

func (h *mheap) alloc_m(npage uintptr, spanclass spanClass, large bool) *mspan {
	_g_ := getg()

	// To prevent excessive heap growth, before allocating n pages
	// we need to sweep and reclaim at least n pages.
	if h.sweepdone == 0 {
		h.reclaim(npage)
	}

	lock(&h.lock)
	// transfer stats from cache to global
	memstats.heap_scan += uint64(_g_.m.mcache.local_scan)
	_g_.m.mcache.local_scan = 0
	memstats.tinyallocs += uint64(_g_.m.mcache.local_tinyallocs)
	_g_.m.mcache.local_tinyallocs = 0

	s := h.allocSpanLocked(npage, &memstats.heap_inuse)
	if s != nil {
		// Record span info, because gc needs to be
		// able to map interior pointer to containing span.
		atomic.Store(&s.sweepgen, h.sweepgen)
		h.sweepSpans[h.sweepgen/2%2].push(s) // Add to swept in-use list.
		s.state = mSpanInUse
		s.allocCount = 0
		s.spanclass = spanclass
		if sizeclass := spanclass.sizeclass(); sizeclass == 0 {
			s.elemsize = s.npages << _PageShift
			s.divShift = 0
			s.divMul = 0
			s.divShift2 = 0
			s.baseMask = 0
		} else {
			s.elemsize = uintptr(class_to_size[sizeclass])
			m := &class_to_divmagic[sizeclass]
			s.divShift = m.shift
			s.divMul = m.mul
			s.divShift2 = m.shift2
			s.baseMask = m.baseMask
		}

		// Mark in-use span in arena page bitmap.
		arena, pageIdx, pageMask := pageIndexOf(s.base())
		arena.pageInUse[pageIdx] |= pageMask

		// update stats, sweep lists
		h.pagesInUse += uint64(npage)
		if large {
			memstats.heap_objects++
			mheap_.largealloc += uint64(s.elemsize)
			mheap_.nlargealloc++
			atomic.Xadd64(&memstats.heap_live, int64(npage<<_PageShift))
		}
	}
	// heap_scan and heap_live were updated.
	if gcBlackenEnabled != 0 {
		gcController.revise()
	}

	if trace.enabled {
		traceHeapAlloc()
	}

	// h.spans is accessed concurrently without synchronization
	// from other threads. Hence, there must be a store/store
	// barrier here to ensure the writes to h.spans above happen
	// before the caller can publish a pointer p to an object
	// allocated from s. As soon as this happens, the garbage
	// collector running on another processor could read p and
	// look up s in h.spans. The unlock acts as the barrier to
	// order these writes. On the read side, the data dependency
	// between p and the index in h.spans orders the reads.
	unlock(&h.lock)
	return s
}
```

`s := h.allocSpanLocked(npage, &memstats.heap_inuse)` 从free区域分配span，并从freelist移除

如果freelist也没有足够空间，则会调用grow()函数，向arena申请mspan, 申请到的mspan插入freelist队尾

```
func (h *mheap) allocSpanLocked(npage uintptr, stat *uint64) *mspan {
	t := h.free.find(npage)
	if t.valid() {
		goto HaveSpan
	}
	if !h.grow(npage) {
		return nil
	}
	t = h.free.find(npage)
	if t.valid() {
		goto HaveSpan
	}
	throw("grew heap, but no adequate free span found")

HaveSpan:
	s := t.span()
	if s.state != mSpanFree {
		throw("candidate mspan for allocation is not free")
	}

	// First, subtract any memory that was released back to
	// the OS from s. We will add back what's left if necessary.
	memstats.heap_released -= uint64(s.released())

	if s.npages == npage {
		h.free.erase(t)
	} else if s.npages > npage {
		// Trim off the lower bits and make that our new span.
		// Do this in-place since this operation does not
		// affect the original span's location in the treap.
		n := (*mspan)(h.spanalloc.alloc())
		h.free.mutate(t, func(s *mspan) {
			n.init(s.base(), npage)
			s.npages -= npage
			s.startAddr = s.base() + npage*pageSize
			h.setSpan(s.base()-1, n)
			h.setSpan(s.base(), s)
			h.setSpan(n.base(), n)
			n.needzero = s.needzero
			// n may not be big enough to actually be scavenged, but that's fine.
			// We still want it to appear to be scavenged so that we can do the
			// right bookkeeping later on in this function (i.e. sysUsed).
			n.scavenged = s.scavenged
			// Check if s is still scavenged.
			if s.scavenged {
				start, end := s.physPageBounds()
				if start < end {
					memstats.heap_released += uint64(end - start)
				} else {
					s.scavenged = false
				}
			}
		})
		s = n
	} else {
		throw("candidate mspan for allocation is too small")
	}
	// "Unscavenge" s only AFTER splitting so that
	// we only sysUsed whatever we actually need.
	if s.scavenged {
		// sysUsed all the pages that are actually available
		// in the span. Note that we don't need to decrement
		// heap_released since we already did so earlier.
		sysUsed(unsafe.Pointer(s.base()), s.npages<<_PageShift)
		s.scavenged = false
	}

	h.setSpans(s.base(), npage, s)

	*stat += uint64(npage << _PageShift)
	memstats.heap_idle -= uint64(npage << _PageShift)

	if s.inList() {
		throw("still in list")
	}
	return s
}
```

grow()函数, 会从当前arena区域获取空间，如果没有足够空间，则进行arena区域扩张。

```
func (h *mheap) grow(npage uintptr) bool {
	ask := npage << _PageShift

	nBase := round(h.curArena.base+ask, physPageSize)
	if nBase > h.curArena.end {
		// Not enough room in the current arena. Allocate more
		// arena space. This may not be contiguous with the
		// current arena, so we have to request the full ask.
		av, asize := h.sysAlloc(ask)
		if av == nil {
			print("runtime: out of memory: cannot allocate ", ask, "-byte block (", memstats.heap_sys, " in use)\n")
			return false
		}

		if uintptr(av) == h.curArena.end {
			// The new space is contiguous with the old
			// space, so just extend the current space.
			h.curArena.end = uintptr(av) + asize
		} else {
			// The new space is discontiguous. Track what
			// remains of the current space and switch to
			// the new space. This should be rare.
			if size := h.curArena.end - h.curArena.base; size != 0 {
				h.growAddSpan(unsafe.Pointer(h.curArena.base), size)
			}
			// Switch to the new space.
			h.curArena.base = uintptr(av)
			h.curArena.end = uintptr(av) + asize
		}

		// The memory just allocated counts as both released
		// and idle, even though it's not yet backed by spans.
		//
		// The allocation is always aligned to the heap arena
		// size which is always > physPageSize, so its safe to
		// just add directly to heap_released. Coalescing, if
		// possible, will also always be correct in terms of
		// accounting, because s.base() must be a physical
		// page boundary.
		memstats.heap_released += uint64(asize)
		memstats.heap_idle += uint64(asize)

		// Recalculate nBase
		nBase = round(h.curArena.base+ask, physPageSize)
	}

	// Grow into the current arena.
	v := h.curArena.base
	h.curArena.base = nBase
	h.growAddSpan(unsafe.Pointer(v), nBase-v)
	return true
}
```

`growAddSpan`函数获得空间，初始化成mspan, 随后插入freelist

```
func (h *mheap) growAddSpan(v unsafe.Pointer, size uintptr) {
   // Scavenge some pages to make up for the virtual memory space
   // we just allocated, but only if we need to.
   h.scavengeIfNeededLocked(size)

   s := (*mspan)(h.spanalloc.alloc())
   s.init(uintptr(v), size/pageSize)
   h.setSpans(s.base(), s.npages, s)
   s.state = mSpanFree
   // [v, v+size) is always in the Prepared state. The new span
   // must be marked scavenged so the allocator transitions it to
   // Ready when allocating from it.
   s.scavenged = true
   // This span is both released and idle, but grow already
   // updated both memstats.
   h.coalesce(s)
   h.free.insert(s)
}
```

`sysAlloc` 函数展示了arena区域扩张过程

```
func (h *mheap) sysAlloc(n uintptr) (v unsafe.Pointer, size uintptr) {
	...
}
```





### 调度原理

> https://blog.csdn.net/u010853261/article/details/84790392





### mallocinit()内存分配器

```
runtime包
1. 设置GC收集百分比
2. 设置P的数量 GOMAXPROC
3. 获取GOARCH、GOOS和GOROOT


1. 内存管理
2. 内存布局
3. go程调度 - GPM 模型
4. channel调度 - 缓冲与非缓冲，如何优雅关闭channel
https://blog.csdn.net/qq_33296108/article/details/82731686
N 个发送者 M个接收者，如何关闭？加一把锁
type MyChannel struct {
	C      chan T
	closed bool
	mutex  sync.Mutex
}
添加一个 停止通知 接收端告诉发送端不要发送了
5. 反射 valueOf，TypeOf， 断言，强转换
6. 启动顺序


```

### GPM模型

```
以前GM模型，从全局队列中获取G程时，频繁上锁，影响性能，所以加入了p(PROC 本地队列)。
多个P队列事先从Goblo队列获取G程，M绑定P队列时就可以执行P队列中的G程，竞争的情况少了许多。

M对应的是 内核线程KSE,一一对应，通过runtime.SetMaxThread设置
P对应的是 上下文，从全局队列获取Goroutine，拿到本地队列中。绑定了M，就开始执行其中的Goroutine。Goroutine如果有阻塞，就让GOroutine处于等待状态。可以通过runtime.MAXPROCESSNUM()
G对应的是 Gotoutine,通过go关键词生成，会划分GOroutine自己的堆栈和内存空间。同时有多个状态。

P与M是抢占式调度。


```

### map、slice

- https://ninokop.github.io/2017/10/24/go-hashmap/
- map
```
参考文章：https://i6448038.github.io/2018/08/26/map-secret/
主要有二个结构
hmap和bmap
bmap相当于bucket
hmap保存2^B个bucket
每一个bucket有8个CELL，每个cell就是一对key=>value

访问的过程：
key 使用aes散列函数或者memhash散列函数，生成一串hash值,hash值的后B位对应的十进制数就是bucket号
根据hash值的高8位对应一个bucket里某一个cell

冲突解决：
golang使用链表的方式，还有一种开放地址法，准备更多的空位

扩容过程：
当装载因子（有数据的位置/总位置）高了，就开始扩容更多位置。一倍一倍开始扩容。
新的hmap会指向旧的hmap，旧的hmap中的数据不会立马迁移到新的hmap中，当旧的hmap中数据被访问时，才会迁移到新的map
```

- slice
```
指针，长度len，容量cap
```


### channel



- https://ninokop.github.io/2017/11/07/go-channel/
- channel数据结构 
```
使用一个环形队列
保存发送者Goroutine队列
保存接受者Goroutine队列

```

- channel目的
```
接受：从一个goroutine中的内存数据拷贝到channel环形队列中
发送：从channel环形队列中拷贝到goroutine内存中
```

- 理解发送阻塞
```
当前goroutine被挂起，并且放到保存发送者Goroutine队列中。等环形队列被消费了，唤醒发送队列保存的goroutine
```

- 如何从channel获取数据
```
for-range
for-select

i,ok := <-ch
当ok为true说明获取到数据,当ok为false说明队列已经关闭

select实现
将每个case后面的管道，放到一起存放在队列中
随机对某一个case求值，如果能获取到数据则执行
否则执行default case，
否则阻塞

```

